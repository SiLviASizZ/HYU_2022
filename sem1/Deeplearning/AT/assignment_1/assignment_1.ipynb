{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zifx8uJqvK24",
        "outputId": "68cc7d26-4428-4bf9-b616-fd7a2b1c878c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch : 1 / 160, cost : 2.255253314971924\n",
            "Epoch : 2 / 160, cost : 2.082606315612793\n",
            "Epoch : 3 / 160, cost : 1.9869563579559326\n",
            "Epoch : 4 / 160, cost : 1.9287168979644775\n",
            "Epoch : 5 / 160, cost : 1.8850489854812622\n",
            "Epoch : 6 / 160, cost : 1.8459514379501343\n",
            "Epoch : 7 / 160, cost : 1.8156145811080933\n",
            "Epoch : 8 / 160, cost : 1.7960020303726196\n",
            "Epoch : 9 / 160, cost : 1.767411231994629\n",
            "Epoch : 10 / 160, cost : 1.7516019344329834\n",
            "Epoch : 11 / 160, cost : 1.7313088178634644\n",
            "Epoch : 12 / 160, cost : 1.7179089784622192\n",
            "Epoch : 13 / 160, cost : 1.7020522356033325\n",
            "Epoch : 14 / 160, cost : 1.6904759407043457\n",
            "Epoch : 15 / 160, cost : 1.6746259927749634\n",
            "Epoch : 16 / 160, cost : 1.6580930948257446\n",
            "Epoch : 17 / 160, cost : 1.647814393043518\n",
            "Epoch : 18 / 160, cost : 1.6379948854446411\n",
            "Epoch : 19 / 160, cost : 1.6269488334655762\n",
            "Epoch : 20 / 160, cost : 1.6143229007720947\n",
            "Epoch : 21 / 160, cost : 1.6042277812957764\n",
            "Epoch : 22 / 160, cost : 1.594563364982605\n",
            "Epoch : 23 / 160, cost : 1.5814378261566162\n",
            "Epoch : 24 / 160, cost : 1.570913553237915\n",
            "Epoch : 25 / 160, cost : 1.5632466077804565\n",
            "Epoch : 26 / 160, cost : 1.5534873008728027\n",
            "Epoch : 27 / 160, cost : 1.547816276550293\n",
            "Epoch : 28 / 160, cost : 1.537484049797058\n",
            "Epoch : 29 / 160, cost : 1.5311012268066406\n",
            "Epoch : 30 / 160, cost : 1.5268769264221191\n",
            "Epoch : 31 / 160, cost : 1.5199710130691528\n",
            "Epoch : 32 / 160, cost : 1.5118082761764526\n",
            "Epoch : 33 / 160, cost : 1.504710078239441\n",
            "Epoch : 34 / 160, cost : 1.496340036392212\n",
            "Epoch : 35 / 160, cost : 1.4879127740859985\n",
            "Epoch : 36 / 160, cost : 1.4859228134155273\n",
            "Epoch : 37 / 160, cost : 1.4723904132843018\n",
            "Epoch : 38 / 160, cost : 1.4708751440048218\n",
            "Epoch : 39 / 160, cost : 1.4673948287963867\n",
            "Epoch : 40 / 160, cost : 1.460572600364685\n",
            "Epoch : 41 / 160, cost : 1.4531959295272827\n",
            "Epoch : 42 / 160, cost : 1.445419192314148\n",
            "Epoch : 43 / 160, cost : 1.4430910348892212\n",
            "Epoch : 44 / 160, cost : 1.4369778633117676\n",
            "Epoch : 45 / 160, cost : 1.4321202039718628\n",
            "Epoch : 46 / 160, cost : 1.4249025583267212\n",
            "Epoch : 47 / 160, cost : 1.4217041730880737\n",
            "Epoch : 48 / 160, cost : 1.4116519689559937\n",
            "Epoch : 49 / 160, cost : 1.4089791774749756\n",
            "Epoch : 50 / 160, cost : 1.3987821340560913\n",
            "Epoch : 51 / 160, cost : 1.396040916442871\n",
            "Epoch : 52 / 160, cost : 1.3933327198028564\n",
            "Epoch : 53 / 160, cost : 1.387923240661621\n",
            "Epoch : 54 / 160, cost : 1.3830277919769287\n",
            "Epoch : 55 / 160, cost : 1.3778656721115112\n",
            "Epoch : 56 / 160, cost : 1.3698159456253052\n",
            "Epoch : 57 / 160, cost : 1.366762399673462\n",
            "Epoch : 58 / 160, cost : 1.363691806793213\n",
            "Epoch : 59 / 160, cost : 1.3578441143035889\n",
            "Epoch : 60 / 160, cost : 1.3541462421417236\n",
            "Epoch : 61 / 160, cost : 1.3508507013320923\n",
            "Epoch : 62 / 160, cost : 1.3459984064102173\n",
            "Epoch : 63 / 160, cost : 1.3378300666809082\n",
            "Epoch : 64 / 160, cost : 1.3396742343902588\n",
            "Epoch : 65 / 160, cost : 1.332438588142395\n",
            "Epoch : 66 / 160, cost : 1.3255192041397095\n",
            "Epoch : 67 / 160, cost : 1.3192962408065796\n",
            "Epoch : 68 / 160, cost : 1.3204694986343384\n",
            "Epoch : 69 / 160, cost : 1.3139840364456177\n",
            "Epoch : 70 / 160, cost : 1.3093397617340088\n",
            "Epoch : 71 / 160, cost : 1.3052312135696411\n",
            "Epoch : 72 / 160, cost : 1.2978172302246094\n",
            "Epoch : 73 / 160, cost : 1.299651026725769\n",
            "Epoch : 74 / 160, cost : 1.2900364398956299\n",
            "Epoch : 75 / 160, cost : 1.291399359703064\n",
            "Epoch : 76 / 160, cost : 1.2901471853256226\n",
            "Epoch : 77 / 160, cost : 1.2822037935256958\n",
            "Epoch : 78 / 160, cost : 1.2789533138275146\n",
            "Epoch : 79 / 160, cost : 1.274908185005188\n",
            "Epoch : 80 / 160, cost : 1.2686779499053955\n",
            "Epoch : 81 / 160, cost : 1.2690904140472412\n",
            "Epoch : 82 / 160, cost : 1.2667052745819092\n",
            "Epoch : 83 / 160, cost : 1.259090781211853\n",
            "Epoch : 84 / 160, cost : 1.2589161396026611\n",
            "Epoch : 85 / 160, cost : 1.2530357837677002\n",
            "Epoch : 86 / 160, cost : 1.2477390766143799\n",
            "Epoch : 87 / 160, cost : 1.2441450357437134\n",
            "Epoch : 88 / 160, cost : 1.2425198554992676\n",
            "Epoch : 89 / 160, cost : 1.2341543436050415\n",
            "Epoch : 90 / 160, cost : 1.232588529586792\n",
            "Epoch : 91 / 160, cost : 1.2324453592300415\n",
            "Epoch : 92 / 160, cost : 1.225429654121399\n",
            "Epoch : 93 / 160, cost : 1.219242811203003\n",
            "Epoch : 94 / 160, cost : 1.2242286205291748\n",
            "Epoch : 95 / 160, cost : 1.2125494480133057\n",
            "Epoch : 96 / 160, cost : 1.2111355066299438\n",
            "Epoch : 97 / 160, cost : 1.2063084840774536\n",
            "Epoch : 98 / 160, cost : 1.2042814493179321\n",
            "Epoch : 99 / 160, cost : 1.206503987312317\n",
            "Epoch : 100 / 160, cost : 1.201784372329712\n",
            "Epoch : 101 / 160, cost : 1.1964893341064453\n",
            "Epoch : 102 / 160, cost : 1.1879416704177856\n",
            "Epoch : 103 / 160, cost : 1.1835968494415283\n",
            "Epoch : 104 / 160, cost : 1.183623194694519\n",
            "Epoch : 105 / 160, cost : 1.181296706199646\n",
            "Epoch : 106 / 160, cost : 1.1768913269042969\n",
            "Epoch : 107 / 160, cost : 1.1789767742156982\n",
            "Epoch : 108 / 160, cost : 1.1773340702056885\n",
            "Epoch : 109 / 160, cost : 1.1676081418991089\n",
            "Epoch : 110 / 160, cost : 1.1628165245056152\n",
            "Epoch : 111 / 160, cost : 1.1610398292541504\n",
            "Epoch : 112 / 160, cost : 1.1585716009140015\n",
            "Epoch : 113 / 160, cost : 1.1537522077560425\n",
            "Epoch : 114 / 160, cost : 1.1511871814727783\n",
            "Epoch : 115 / 160, cost : 1.148437738418579\n",
            "Epoch : 116 / 160, cost : 1.1441402435302734\n",
            "Epoch : 117 / 160, cost : 1.145243763923645\n",
            "Epoch : 118 / 160, cost : 1.1389986276626587\n",
            "Epoch : 119 / 160, cost : 1.1384525299072266\n",
            "Epoch : 120 / 160, cost : 1.1273043155670166\n",
            "Epoch : 121 / 160, cost : 1.1281630992889404\n",
            "Epoch : 122 / 160, cost : 1.1258829832077026\n",
            "Epoch : 123 / 160, cost : 1.119233250617981\n",
            "Epoch : 124 / 160, cost : 1.1163060665130615\n",
            "Epoch : 125 / 160, cost : 1.120285987854004\n",
            "Epoch : 126 / 160, cost : 1.1154274940490723\n",
            "Epoch : 127 / 160, cost : 1.1075313091278076\n",
            "Epoch : 128 / 160, cost : 1.1070350408554077\n",
            "Epoch : 129 / 160, cost : 1.1050796508789062\n",
            "Epoch : 130 / 160, cost : 1.0995742082595825\n",
            "Epoch : 131 / 160, cost : 1.0967636108398438\n",
            "Epoch : 132 / 160, cost : 1.0950199365615845\n",
            "Epoch : 133 / 160, cost : 1.0929298400878906\n",
            "Epoch : 134 / 160, cost : 1.085394024848938\n",
            "Epoch : 135 / 160, cost : 1.086136817932129\n",
            "Epoch : 136 / 160, cost : 1.0856260061264038\n",
            "Epoch : 137 / 160, cost : 1.0741064548492432\n",
            "Epoch : 138 / 160, cost : 1.0778381824493408\n",
            "Epoch : 139 / 160, cost : 1.0783816576004028\n",
            "Epoch : 140 / 160, cost : 1.0679621696472168\n",
            "Epoch : 141 / 160, cost : 1.0683687925338745\n",
            "Epoch : 142 / 160, cost : 1.062306523323059\n",
            "Epoch : 143 / 160, cost : 1.061970829963684\n",
            "Epoch : 144 / 160, cost : 1.0616629123687744\n",
            "Epoch : 145 / 160, cost : 1.0509498119354248\n",
            "Epoch : 146 / 160, cost : 1.0589803457260132\n",
            "Epoch : 147 / 160, cost : 1.0511521100997925\n",
            "Epoch : 148 / 160, cost : 1.0453178882598877\n",
            "Epoch : 149 / 160, cost : 1.0436102151870728\n",
            "Epoch : 150 / 160, cost : 1.0365651845932007\n",
            "Epoch : 151 / 160, cost : 1.0404207706451416\n",
            "Epoch : 152 / 160, cost : 1.0328470468521118\n",
            "Epoch : 153 / 160, cost : 1.0338952541351318\n",
            "Epoch : 154 / 160, cost : 1.0330097675323486\n",
            "Epoch : 155 / 160, cost : 1.021228313446045\n",
            "Epoch : 156 / 160, cost : 1.0232656002044678\n",
            "Epoch : 157 / 160, cost : 1.0158047676086426\n",
            "Epoch : 158 / 160, cost : 1.019159197807312\n",
            "Epoch : 159 / 160, cost : 1.0143619775772095\n",
            "Epoch : 160 / 160, cost : 1.0119454860687256\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 미리 작성된 코드들은 수정할 수 없으며, 이외의 코드를 작성하시면 됩니다.\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    # 모델의 코드는 여기서 작성해주세요\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(32*32*3, 512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.linear3 = nn.Linear(256, 64)\n",
        "        self.linear4 = nn.Linear(64, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.15) # dropout 0.15=>0.5\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        a1 = self.activation(z1)\n",
        "        a1 = self.dropout(a1)\n",
        "\n",
        "        z2 = self.linear2(a1)\n",
        "        a2 = self.activation(z2)\n",
        "        a2 = self.dropout(a2)\n",
        "\n",
        "        z3 = self.linear3(a2)\n",
        "        a3 = self.activation(z3)\n",
        "        a3 = self.dropout(a3)\n",
        "\n",
        "        z4 = self.linear4(a3)\n",
        "        \n",
        "        return z4\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 학습코드는 모두 여기서 작성해주세요\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                 train=True,\n",
        "                                                 transform=transforms.ToTensor(),\n",
        "                                                 download=True)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                                train=False,\n",
        "                                                transform=transforms.ToTensor(),\n",
        "                                                download=True)\n",
        "\n",
        "    batch_size = 512\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle = True )\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size)\n",
        "    \n",
        "    model = Classifier().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005) # optim?\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #===================================\n",
        "    #            train code \n",
        "    #===================================\n",
        "\n",
        "    epochs = 160\n",
        "    lmbd = 0.1\n",
        "    train_avg_costs = []\n",
        "    test_avg_costs = []\n",
        "\n",
        "    test_total_batch = len(test_dataloader)\n",
        "    total_batch_num = len(train_dataloader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        model.train()\n",
        "        for b_x, b_y in train_dataloader:\n",
        "          b_x = b_x.view(-1, 32*32*3).to(device)\n",
        "          logits = model(b_x) # forward propagation\n",
        "          loss = criterion(logits, b_y.to(device)) # get cost\n",
        "\n",
        "          reg = model.linear1.weight.pow(2.0).sum()\n",
        "          reg += model.linear2.weight.pow(2.0).sum()\n",
        "          reg += model.linear3.weight.pow(2.0).sum()\n",
        "          reg += model.linear4.weight.pow(2.0).sum()\n",
        "\n",
        "          loss += lmbd*reg/len(b_x)/2.\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward() # back propagation\n",
        "          optimizer.step() # update parameters\n",
        "\n",
        "          avg_cost += loss / total_batch_num\n",
        "        train_avg_costs.append(avg_cost.detach()) # point \n",
        "        print('Epoch : {} / {}, cost : {}'.format(epoch+1, epochs, avg_cost))\n",
        "\n",
        "        test_avg_cost = 0\n",
        "        model.eval()\n",
        "        for b_x, b_y in test_dataloader:\n",
        "          b_x = b_x.view(-1, 32*32*3).to(device)\n",
        "          with torch.no_grad():\n",
        "            logits = model(b_x)\n",
        "            test_loss = criterion(logits, b_y.to(device))\n",
        "          test_avg_cost += test_loss / test_total_batch\n",
        "        \n",
        "        test_avg_costs.append(test_avg_cost.detach()) # point\n",
        "\n",
        "        # epoch = range(epochs)\n",
        "        # plt.plot(epoch.train_avg_costs,'r-')\n",
        "        # plt.plot(epoch.test_avg_costs,'b-')\n",
        "        # plt.xlabel(\"Epoch\")\n",
        "        # plt.ylabel(\"Loss\")\n",
        "        # plt.legend(['train','test'])\n",
        "        # plt.show()\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), 'model.pt')  # 학습된 모델을 저장하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1BHoIEivAIf",
        "outputId": "51c41849-d53e-44ab-af45-3318439ec3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test set : 56.1200%\n"
          ]
        }
      ],
      "source": [
        "# 학습된 모델의 성능을 평가하는 코드입니다.\n",
        "# 아래의 코드로 평가를 진행할 예정이므로 아래의 코드가 정상 동작 해야하며, 제출전 모델의 성능을 확인하시면 됩니다.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"CIFAR10/\",\n",
        "                                            train=False,\n",
        "                                            transform=transforms.ToTensor(),\n",
        "                                            download=True)\n",
        "\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10000)\n",
        "\n",
        "classifier = Classifier().to(device)\n",
        "classifier.load_state_dict(torch.load('model.pt'))\n",
        "classifier.eval()\n",
        "\n",
        "\n",
        "for data, label in test_dataloader:\n",
        "    data = data.view(-1, 32 * 32 * 3).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(data)\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        total = len(label)\n",
        "        correct = torch.eq(pred, label.to(device)).sum()\n",
        "\n",
        "        print(\"Accuracy on test set : {:.4f}%\".format(100 * correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}