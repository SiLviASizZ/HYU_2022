요약 : 드노보를 활용한 펩타이드 시퀀싱 ( 펩타이드 서열 분석 )
스펙트럼의 m/z 및 intensity 등의 고유한 특징을 이용하여 펩타이드를 분석하는 방법.


스펙트럼의 고유한 특징 ) m/z , intensity
이것들을 이용하여 peptide 를 분석 = 드노보를 활용한 펩타이드 시퀀싱

Transformer 모델을 활용 .

1. 서론
단백질 서열 정보를 얻고자 함 => 펩타이드 서열 정보로 대체
대체하는 과정 ) Digestion ( 소화 ) => ionization ( 이온화 ) => 질량 분석기, 텐덤 질량 스펙트럼 이용하여 펩타이드 서열 분석

펩타이드 분석의 두 가지 방법 - Database 탐색 , De novo . 

2. 본론
Transformer : Attention method 로 구현된 모델
transformer input : sentence , output : sentence . => 데이터를 input form 에 맞게 가공
m/z , intensity : 데이터로 주어졌다
m/z 를 포지셔널 임베딩 => intensity 와 곱해서 스펙트럼 데이터 생성 => input






------- 조사할 것 -------

1. De novo 를 활용한 펩타이드 시퀀싱 개요 , m/z intensity 란 무엇인가
2. Transformer 모델이란 ? + CNN RNN LSTM 에 관한 간략한 설명
3. 단백질 서열정보 / 펩타이드 서열정보 => 서열의 의미 ?
4. Digestion = ? , ionization = ? , 질량 분석기, 텐덤 질량 스펙트럼 = ?
5. positional embedding = ?

6. 참고문헌 
[1] Steen, Hanno, and Matthias Mann. "The ABC's (and XYZ's) of peptide sequencing." Nature reviews Molecular cell biology 5.9: 699-711, 2004.
[2] MacCoss, Michael J., Christine C. Wu, and John R. Yates. "Probability-based validation of protein identifications using a modified SEQUEST algorithm." Analytical chemistry 74.21 : 5593-5599, 2002.
[3] Kim, Sangtae, et al. "Spectral dictionaries: Integrating de novo peptide sequencing with database search of tandem mass spectra." Molecular & Cellular Proteomics 8.1 : 53-69, 2009.
[4] Tran, Ngoc Hieu, et al. “De novo peptide sequencing by deep learning.” Proceedings of the National Academy of Sciences:8247-8252, 2017.


-------------------------내용--------------------------------
1. De novo 를 활용한 펩타이드 시퀀싱 개요 , what is m/z , intensity ?

2. what is Transformer Model ?
2017년 구글 연구팀이 발표한 논문에서 제안된 모델.
Attention Mechanism 사용 . + 임베딩 층, 밀집 층, 정규화 층, Dense 층 등 ...

- Attention Mechanism

 

  


	1) 인코더 : input = 단어 ID의 sequence 로 표현된 문장의 batch ( batch size , max length )
	인코더 : 각 단어를 512 dimension 의 표현으로 인코딩 , output 출력 [batch size , max length , 512]
	Nx : 쌓아 올린다 ?

	2) 디코더 : input : target sentence while training ( 단어 ID의 sequence 로 표현됨 )
	SOS token added => input ( 단어 ID의 sequence ) : time step 이 오른쪽으로 하나 이동된 상태
	인코더의 출력을 받는데, N번의 디코더에 모두 주입됨
	time step 마다, 가능한 다음 단어에 대한 확률을 출력
	출력 크기는 [Batch size, max length, 어휘 사전 길이]

	3) 추론 시에는 디코더에 타깃을 주입할 수 없음.
	
	4) 구성 요소 :
	임베딩 층 2개 , 스킵 연결 5 * N 개 , 피드포워드 모듈 2 * N 개 ( consists of 정규화 층, 밀집 층 2개 <  ReLU 활성화 함수 사용 여부 차이 있음 )
	마지막 출력 층 : 밀집 층 ( 소프트맥스 활성화 함수 사용 )

	모든 층은 타임 스텝에 대하여 독립적임 ( time-distributed ) => 각 단어는 다른 모든 단어에 대해 독립적으로 처리.
	
	- 인코더의 Multi-head attention 층 ) 각 단어와 동일한 문장에 있는 다른 단어의 관계를 인코딩 ( self-attention )
	- 디코더의 Masked multi-head attention 층 도 동일한 작업을 수행 , but 각 단어는 이전에 등장한 단어에만 attention 할 수 있음
	  디코더의 Multi-head attention 층 => 디코더가 입력 문장에 있는 단어에 대해 attention

	5) positional encoding : 문장에 있는 단어의 위치를 나타내는 밀집 벡터
	n번째 위치 인코딩이 각 문장에 있는 n번째 단어의 단어 임베딩에 더해짐 => model 이 각 단어의 위치를 알 수 있음 !!

	- Multi-head attention 층은 단어의 순서나 위치를 고려하지 않음 + 다른 층들은 time-distributed => 각 단어의 위치를 알 수 없음
	
	
